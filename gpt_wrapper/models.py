from typing import Optional, Callable
from collections.abc import Iterator, Generator
from abc import ABC, abstractmethod
from dataclasses import dataclass
import asyncio

from .api import openai_chat, accumulate_partial, NOT_GIVEN, ChatCompletion, ChatCompletionMessage, ToolCall
from .messages import MessageHistory, msg
from .tools import Tools, call_requested_function
from .utils import atuple


#================ Assistant =================#

class Assistant(ABC):
    '''Generic AI Assistant with memory and function calling high-level overridable methods and hooks for flexible customization'''
    class Event(ABC):
        '''Event base class'''
        pass

    @dataclass
    class ResponseStart(Event):
        '''Assistant was called with the following args'''
        user: str
        tools: Optional[Tools]
        model: str
        max_function_calls: int
        openai_kwargs: dict
    
    @dataclass
    class Completion(Event):
        '''Model returned response to API call'''
        completion: ChatCompletion
        call_index: int = 0 # i-th response in this prompt
    
    @dataclass
    class ChatMessage(Event):
        '''One message generated by the model, i.e. choice[i]'''
        message: ChatCompletionMessage
        choice_index: int = 0 # index of this message in the choices list
    
    @dataclass
    class TextMessage(Event):
        '''Pure text response part of the message'''
        content: str
    
    @dataclass
    class ToolCalls(Event):
        '''(One or more) tool calls generated by the model'''
        tool_calls: list[ToolCall]
    
    @dataclass
    class ToolResult(Event):
        '''Result of a tool call'''
        result: str
        tool_call: ToolCall
        index: int = 0 # index of this tool call in the tool_calls list

    @dataclass
    class ResponseEnd(Event):
        '''Response is complete'''
        content: str
    
    @dataclass
    class MaxCallsExceeded(Event):
        '''Maximum number of function calls exceeded'''
        num_calls: int

    @dataclass
    class MaxTokensExceeded(Event):
        '''Maximum number of tokens exceeded'''
        num_tokens: int


class ChatGPT(Assistant):
    '''ChatGPT with default model and toolkit'''
    def __init__(self, messages: MessageHistory, tools: Optional[Tools] = None, model: str = 'gpt-3.5-turbo'):
        self.default_model = model
        self.default_tools = tools

        self.messages = messages

    #================ High-Level Functions =================#
    async def execute_tool(self, call: ToolCall, tools: Tools) -> str:
        '''Execute the requested tool call and return its result.'''
        result = await call_requested_function(call.function, tools.lookup)
        result = str(result)
        return result
    
    async def handle_stream(self, stream) -> ChatCompletion:
        async for chunk, completion in accumulate_partial(stream):
            pass
        return completion
    

    #================ Event Handler =================#
    async def __call__(self, user: str, tools: Optional[Tools] = None, model: Optional[str] = None, max_function_calls: int = 100, **openai_kwargs) -> str:
        '''Generate a chat response from a user, returning the final response.'''
        async for event in self.response_events(user, tools, model, max_function_calls, **openai_kwargs):
            match type(event):
                case self.ResponseStart:
                    event: self.ResponseStart
                    print(f"[Chat Start]: {event.user}")

                case self.TextMessage:
                    event: self.TextMessage
                    print(f"[Text Message]: {event.content}")
                
                case self.ToolCalls:
                    event: self.ToolCalls
                    print(f"[Tool Calls]: {event.tool_calls}")

                case self.ToolResult:
                    event: self.ToolResult
                    print(f"[Tool Result]: {event.result}")
                
                case self.ResponseEnd:
                    event: self.ResponseEnd
                    print(f"[Chat End]: {event.content}")
                    return event.content

                case _:
                    print(f"Unhandled Event: {event}")

    
    async def response_events(self, user: str, tools: Optional[Tools] = None, model: Optional[str] = None, max_function_calls: int = 100, parallel_calls=True, cancel_stream: Optional[asyncio.Event] = None, **openai_kwargs) -> Iterator[Assistant.Event]:
        '''Generate events from a chat, yielding each message and tool call as it is received.'''
        model = model or self.default_model
        tools = tools or self.default_tools

        yield self.ResponseStart(user, tools, model, max_function_calls, openai_kwargs)
        self.messages.append(msg(user=user))

        for call_index in range(max_function_calls):
            completion_stream = await openai_chat(
                messages=self.messages.history,
                tools=tools.schema if tools else NOT_GIVEN,
                model=model,
                stream=True,
                **openai_kwargs
            )
            completion = await self.handle_stream(completion_stream)

            # completion stream has been completed
            yield self.Completion(completion, call_index)

            # select the message and handle it
            message = completion.choices[0].message
            self.messages.append(message)
            yield self.ChatMessage(message, choice_index=0)

            if message.content:
                # handle message
                yield self.TextMessage(message.content)

            if tools and message.tool_calls:
                # handle tool calls
                yield self.ToolCalls(message.tool_calls)

                # awaitables for each tool call
                calls = [
                    atuple(i, call, self.execute_tool(call, tools))
                    for i, call in enumerate(message.tool_calls)
                ]
                calls = asyncio.as_completed(calls) if parallel_calls else calls
                for completed in calls:
                    i, call, result = await completed
                    self.messages.append(msg(tool=result, tool_call_id=call.id))
                    yield self.ToolResult(result, call, i)
            else:
                # IMPORTANT no more tool calls, we're done, return the final response
                yield self.ResponseEnd(message.content)
                return
            
        yield self.MaxCallsExceeded(max_function_calls)
    