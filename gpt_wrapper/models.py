from typing import Optional, Callable, AsyncIterator
from collections.abc import Iterator, Generator
from abc import ABC, abstractmethod
from dataclasses import dataclass
import asyncio

from .api import openai_chat, accumulate_partial, NOT_GIVEN, ChatCompletion, ChatCompletionMessage, ToolCall, AsyncStream, ChatCompletionChunk
from .messages import MessageHistory, msg
from .tools import Tools, call_requested_function
from .utils import atuple


#================ Assistant =================#

class Assistant(ABC):
    '''
    Generic AI Assistant with memory and function calling capabilities.
    Uses an event-based system and offers high-level overridable methods for flexible customization.
    You can essentially iterate over the events and only handle the ones you care about.
    You can also override the *_events() generators to emit your own events.
    For passing events to a frontend, you could create an "adapter" generator that converts the events to whichever format the frontend expects, and simply use that generator for Server-Sent Events or StreamedResponses.
    '''
    class Event(ABC):
        '''Event base class for Assistant events'''
        pass

    #================ Full Response Events =================#
    @dataclass
    class ResponseStartEvent(Event):
        '''Assistant was called with the following args'''
        prompt: str
        tools: Optional[Tools]
        model: str
        max_function_calls: int
        openai_kwargs: dict
    
    @dataclass
    class CompletionEvent(Event):
        '''Final response from the API call to the model'''
        completion: ChatCompletion
        call_index: int = 0 # i-th response in this prompt
    
    @dataclass
    class FullMessageEvent(Event):
        '''One message generated by the model, i.e. choice[i]'''
        message: ChatCompletionMessage
        choice_index: int = 0 # index of this message in the choices list
    
    @dataclass
    class TextMessageEvent(Event):
        '''Pure text response part of the message'''
        content: str
    
    @dataclass
    class ToolCallsEvent(Event):
        '''(One or more) tool calls generated by the model'''
        tool_calls: list[ToolCall]
    
    @dataclass
    class ToolResultEvent(Event):
        '''Result of a tool call'''
        result: str
        tool_call: ToolCall
        index: int = 0 # index of this tool call in the tool_calls list

    @dataclass
    class ResponseEndEvent(Event):
        '''Response is complete'''
        content: str

    #================ Partial Response (Streaming) Events =================#
    @dataclass
    class PartialCompletionEvent(Event):
        '''Partial response during streaming, with `chunk` being the direct delta and `partial` being the accumulated response so far'''
        chunk: ChatCompletionChunk
        partial: ChatCompletion
        call_index: int = 0
    
    # TODO: PartialMessage, PartialTextMessage, PartialToolCalls
    # but should these event only yield when its updated? or should it yield every time if it exists?

    #================ Error Events =================#
    @dataclass
    class MaxCallsExceededEvent(Event):
        '''Maximum number of function calls exceeded'''
        num_calls: int

    @dataclass
    class MaxTokensExceededEvent(Event):
        '''Maximum number of tokens exceeded'''
        num_tokens: int
    
    @dataclass
    class ModelTimeoutEvent(Event):
        '''Model timed out'''
    
    @dataclass
    class ToolTimeoutEvent(Event):
        '''Tool call timed out'''
        tool_call: ToolCall
    


class ChatGPT(Assistant):
    '''ChatGPT with default model and toolkit'''
    def __init__(self, messages: MessageHistory, tools: Optional[Tools] = None, model: str = 'gpt-3.5-turbo'):
        self.default_model = model
        self.default_tools = tools

        self.messages = messages
    

    #================ Event Handlers =================#
    async def __call__(self, prompt: str, tools: Optional[Tools] = None, model: Optional[str] = None, max_function_calls: int = 100, **openai_kwargs) -> str:
        '''Prompt the assistant, returning its final response.'''
        async for event in self.response_events(prompt, tools, model, max_function_calls, **openai_kwargs):
            match event:
                case self.ResponseStartEvent():
                    print(f"[User]: {event.prompt}", flush=True)

                case self.TextMessageEvent():
                    print(f"[ChatGPT]: {event.content}", flush=True)
                
                case self.ToolCallsEvent():
                    print(f"[Tool Calls]: {event.tool_calls}", flush=True)

                case self.ToolResultEvent():
                    print(f"[Tool Result]: {event.result}", flush=True)
                
                case self.ResponseEndEvent():
                    print(f"[Final Response]: {event.content}", flush=True)
                    return event.content

                case unhandled_event:
                    # print(f"Unhandled Event: {unhandled_event}")
                    pass


    #================ Event Generators =================#
    async def response_events(self, prompt: str, tools: Optional[Tools] = None, model: Optional[str] = None, max_function_calls: int = 100, parallel_calls = True, **openai_kwargs) -> Iterator[Assistant.Event]:
        '''
        Generate events from a single user prompt, yielding each message and tool call and everything else as it is received.
        This is the overall generalized form, and you would likely only override the smaller *_events() generators to customize each aspect instead.
        '''
        model = model or self.default_model
        tools = tools or self.default_tools

        yield self.ResponseStartEvent(prompt, tools, model, max_function_calls, openai_kwargs)
        self.messages.append(msg(user=prompt))

        # prompt the assistant and let it use the tools
        for call_index in range(max_function_calls):
            # call the underlying API and handle the events
            completion_events = self.completion_events(
                call_index,
                messages=self.messages.history,
                tools=tools.schema if tools else NOT_GIVEN,
                model=model,
                **openai_kwargs
            )
            completion: ChatCompletion = None
            async for completion_event in completion_events:
                yield completion_event
                match completion_event:
                    case self.CompletionEvent():
                        completion = completion_event.completion

            assert completion is not None, "Full Model Completion was not yielded by the completion_events() generator"

            # select the message if there are multiple choices
            message = completion.choices[0].message
            self.messages.append(message)
            yield self.FullMessageEvent(message, choice_index=0)

            if message.content is not None:
                # handle text message content
                yield self.TextMessageEvent(message.content)

            if tools and message.tool_calls:
                # handle tool calls
                async for tool_event in self.tool_events(message.tool_calls, tools, parallel_calls):
                    yield tool_event
            else:
                # IMPORTANT no more tool calls, we're done, return the final response
                yield self.ResponseEndEvent(message.content)
                return
            
        yield self.MaxCallsExceededEvent(max_function_calls)
    

    async def completion_events(self, call_index: int, **openai_kwargs) -> AsyncIterator[Assistant.Event]:
        '''
        Model API call to generate the response from the prompt, always yielding the full completion object at the end.
        This implementation does streaming generation, yielding partial completions as they come in.
        You should override this to customize the stream handling, also defining your own events to yield.
        '''
        # call the underlying API
        completion_stream = await openai_chat(stream=True, **openai_kwargs)

        # yield events during streaming
        async for chunk, partial in accumulate_partial(completion_stream):
            yield self.PartialCompletionEvent(chunk, partial, call_index)

        # The "partial" completion should be fully complete by now
        yield self.CompletionEvent(completion=partial, call_index=call_index)
    

    async def tool_events(self, tool_calls: list[ToolCall], tools: Tools, parallel_calls: bool) -> AsyncIterator[Assistant.Event]:
        '''
        Generate events from a list of tool calls, yielding each tool call, executing them, and yielding the result. You should override this to customize the tool call handling, also defining your own events to yield.
        '''
        yield self.ToolCallsEvent(tool_calls)

        # awaitables for each tool call
        calls = [
            atuple(i, call, call_requested_function(call.function, tools.lookup))
            for i, call in enumerate(tool_calls)
        ]
        # handle each call results as they come in (or in order)
        calls = asyncio.as_completed(calls) if parallel_calls else calls
        for completed in calls:
            i, call, result = await completed
            # stringify the result
            result = str(result)
            self.messages.append(msg(tool=result, tool_call_id=call.id))
            yield self.ToolResultEvent(result, call, i)