{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "A tour of this package's functionality! Buckle up because it's gonna be a fun one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Toolkits\n",
    "\n",
    "### Tools: `@function_tool`\n",
    "You can make any function into a tool usable by the model by decorating it with `@function_tool`, but you must do the following:\n",
    "- The parameters must be type annotated\n",
    "- The return type *should* be str, but not required (currently just stringified)\n",
    "- It must be documented with a `'''docstring'''`, including each parameter (most [formats supported](https://github.com/rr-/docstring_parser), e.g. [Google-style](https://gist.github.com/redlotus/3bc387c2591e3e908c9b63b97b11d24e#file-docstrings-py-L67), [NumPy-style](https://gist.github.com/eikonomega/910512d92769b0cc382a09ae4de41771), sphinx-style, etc, see [this overview](https://gist.github.com/nipunsadvilkar/fec9d2a40f9c83ea7fd97be59261c400))\n",
    "\n",
    "You can use the tool from python as you normally would, but also the annotated tool will be auto-parsed and its JSON schema and argument validator (using pydantic) will be available as attributes on the function:\n",
    "- `tool.schema` is the (singular list of) JSON schema, which you can directly pass to the OpenAI API.\n",
    "- `tool.lookup` is a dict lookup table of the function name to the (auto-validated) function call, meaning you can just receive the model-generated function call and pass it directly to the tool.\n",
    "\n",
    "Other attributes (probably commonly used):\n",
    "- `tool.validator` is the pydantic validator, callable with kwargs to validate the arguments. This is what is saved in the lookup table.\n",
    "- `tool.tool_enabled` is a bool switch to enable/disable the tool. This is useful esp. for Toolkits, where you can dynamically enable/disable tools based on the context. Note that this only works if you wrap the tool in `ToolList` class or is part of a `Toolkit`.\n",
    "- `tool.name` should ideally not be set directly, but passed as a kwarg to `@function_tool`. It is the name of the tool, as known to the model. If not set, it will be the name of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.tools import ToolList, Toolkit, function_tool, fail_with_message\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from python!\n",
      "Hello from GPT!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@function_tool\n",
    "def print_to_console(text: str) -> str:\n",
    "    '''\n",
    "    Print text to console\n",
    "\n",
    "    Args:\n",
    "        text: text to print\n",
    "    '''\n",
    "    print(text)\n",
    "    return 'success' # ideally, we always return something to tell the model\n",
    "\n",
    "# normal call\n",
    "print_to_console('Hello from python!')\n",
    "\n",
    "# call from lookup table\n",
    "lookup = print_to_console.lookup\n",
    "# this would be generated by GPT\n",
    "name, arguments = 'print_to_console', {'text': 'Hello from GPT!'}\n",
    "func = lookup[name]\n",
    "func(arguments) # notice the lack of unpacking, we pass the args dict directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awaiting thread\n",
      "Hello from GPT!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import json\n",
    "from gpt_wrapper.tools import call_requested_function\n",
    "\n",
    "call = SimpleNamespace(name='print_to_console', arguments=json.dumps({'text': 'Hello from GPT!'}))\n",
    "await call_requested_function(call, print_to_console.lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Argument: 1 validation error for print_to_console\n",
      "text\n",
      "  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.5/v/string_type\n",
      "Invalid Argument: 1 validation error for print_to_console\n",
      "text\n",
      "  Field required [type=missing, input_value={'content': 'Whats up?'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.5/v/missing\n"
     ]
    }
   ],
   "source": [
    "# if the model generates an invalid argument, it is auto-validated\n",
    "print(func({'text': 123}))\n",
    "print(func({'content': \"Whats up?\"}))\n",
    "\n",
    "# notice that it's returned as string, because it should be passed to the model to correct itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing off some more goodies:\n",
    "- Even async functions should seamlessly work, just don't forget to `await` them.\n",
    "- `@fail_with_message(err)` is a decorator that will catch any exceptions thrown by the function and instead return the error message. This is useful for when you want to handle errors in a more graceful way than just crashing the model. It also takes an optional logger, which by default takes the `print` function, but any callable that takes a string will work, such as `logger.error` from the `logging` module.\n",
    "- Usually, the `@function_tool` decorator will throw an assertion error if you forget to provide the description for any of the function or their parameters. If you really don't want to provide descriptions for some (or all), maybe because it's so self-explanatory or you need to save tokens, then you can explicitly turn off the docstring parsing by passing `@function_tool(check_description=False)`. This is not recommended, but it's there if you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tool call fib(n=-10) failed: n must be >= 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: n must be >= 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'Fibonacci',\n",
       "   'description': 'Calculate the nth Fibonacci number',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'n': {'description': 'The index of the Fibonacci number to calculate',\n",
       "      'type': 'integer'}},\n",
       "    'required': ['n']}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "@function_tool(name=\"Fibonacci\")\n",
    "@fail_with_message(\"Error\", logger=logger.error)\n",
    "async def fib(n: int):\n",
    "    '''\n",
    "    Calculate the nth Fibonacci number\n",
    "\n",
    "    Args:\n",
    "        n: The index of the Fibonacci number to calculate\n",
    "    '''\n",
    "    if n < 0:\n",
    "         raise ValueError(\"n must be >= 0\")\n",
    "    if n < 2:\n",
    "        return n\n",
    "    await asyncio.sleep(0.1)\n",
    "    # return await fib(n-1) + await fib(n-2)\n",
    "    # parallel\n",
    "    return sum(await asyncio.gather(fib(n-1), fib(n-2)))\n",
    "\n",
    "print(await fib(10), flush=True)\n",
    "print(await fib.lookup['Fibonacci']({'n': -10}))\n",
    "fib.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkits: `class Toolkit`\n",
    "Toolkits are a collection of related function tools, esp. useful when they share a state. Also good for keeping the state bound to a single instance of the toolkit, rather than a global state.\n",
    "To create a toolkit, simply subclass `Toolkit` and decorate its methods with `@function_tool`.\n",
    "Enabled tools will be visible in the attributes:\n",
    "- `toolkit.schema` is the JSON schema, which you can directly pass to the OpenAI API.\n",
    "- `toolkit.lookup` is a dict lookup table of the function name to the (auto-validated) function call, meaning you can just receive the model-generated function call and pass it directly to the tool.\n",
    "\n",
    "As you noticed, the interface for OpenAI API is the same for both tools and toolkits, so you can use them interchangeably, and to use multiple tools and toolkits, simply merge their schemas and lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "class Notepad(Toolkit):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.content = \"<Fill me in>\"\n",
    "    \n",
    "    @function_tool\n",
    "    def write(self, text: str):\n",
    "        '''\n",
    "        Write text to the notepad\n",
    "\n",
    "        Args:\n",
    "            text: The text to write\n",
    "        '''\n",
    "        self.content = text\n",
    "    \n",
    "    @function_tool(check_description=False)\n",
    "    def read(self):\n",
    "        return self.content\n",
    "    \n",
    "notes = Notepad()\n",
    "notes.write(\"Hello, world!\")\n",
    "print(notes.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shhh... here's a secret: 42\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.lookup['write']({'text': \"Shhh... here's a secret: 42\"})\n",
    "notes.lookup['read']({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT model and MessageHistory\n",
    "\n",
    "### ChatGPT: `class ChatGPT`\n",
    "\n",
    "Simple wrapper that keeps a message history object and optional toolkit (TODO: make into list of toolkits).\n",
    "The object is callable and simply takes a user prompt string and returns the model response string.\n",
    "\n",
    "### MessageHistory: `class MessageHistory`\n",
    "\n",
    "A simple class that keeps track of the message history, including the user and assistnat messages, along with system and tool call results. Special subclasses could be defined for things like:\n",
    "- Rolling history (e.g. last 10 messages)\n",
    "- Dynamic history based on context (e.g. vector embedding)\n",
    "- System message that constantly updates (e.g. time, weather, etc.)\n",
    "- Few-shot learning, i.e. pre-populating the history with some demonstration messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.models import ChatGPT\n",
    "from gpt_wrapper.messages import MessageHistory, msg\n",
    "\n",
    "gpt = ChatGPT(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    tools = ToolList(notes, print_to_console, fib),\n",
    "    messages = MessageHistory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[User]: What's on my notepad?\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_5ZEDbC9xRELxX62MQgrt6DIb', function=Function(arguments='{}', name='read'), type='function')]\n",
      "awaiting thread\n",
      "[Tool Result]: Shhh... here's a secret: 42\n",
      "[ChatGPT]: On your notepad, you have the number 42 written as a secret.\n",
      "[Final Response]: On your notepad, you have the number 42 written as a secret.\n",
      "On your notepad, you have the number 42 written as a secret.\n"
     ]
    }
   ],
   "source": [
    "response = await gpt(\"What's on my notepad?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[User]: Can you calculate the 8th fibonacci number, add it to the number in my notes, and write it? also print it to console as well.\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_5ZEDbC9xRELxX62MQgrt6DIb', function=Function(arguments='{\\n  \"n\": 8\\n}', name='Fibonacci'), type='function')]\n",
      "awaiting coroutine\n",
      "[Tool Result]: 21\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_Whczy5fQYmrgelRtDYzGZ5Xx', function=Function(arguments='{\\n  \"text\": \"63\"\\n}', name='write'), type='function')]\n",
      "awaiting thread\n",
      "[Tool Result]: None\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_AaivIquM9AfuSW1dMi3GXyQf', function=Function(arguments='{\\n  \"text\": \"63\"\\n}', name='print_to_console'), type='function')]\n",
      "awaiting thread\n",
      "63\n",
      "[Tool Result]: success\n",
      "[ChatGPT]: I have calculated the 8th Fibonacci number, which is 21. I have added it to the number on your notepad, which is 42, and the result is 63. I have written this number on your notepad and printed it to the console as well.\n",
      "[Final Response]: I have calculated the 8th Fibonacci number, which is 21. I have added it to the number on your notepad, which is 42, and the result is 63. I have written this number on your notepad and printed it to the console as well.\n"
     ]
    }
   ],
   "source": [
    "response = await gpt(\"Can you calculate the 8th fibonacci number, add it to the number in my notes, and write it? also print it to console as well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'63'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user', 'content': \"What's on my notepad?\"},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_5ZEDbC9xRELxX62MQgrt6DIb',\n",
       "    'function': {'arguments': '{}', 'name': 'read'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': \"Shhh... here's a secret: 42\",\n",
       "  'tool_call_id': 'call_5ZEDbC9xRELxX62MQgrt6DIb'},\n",
       " {'content': 'On your notepad, you have the number 42 written as a secret.',\n",
       "  'role': 'assistant'},\n",
       " {'role': 'user',\n",
       "  'content': 'Can you calculate the 8th fibonacci number, add it to the number in my notes, and write it? also print it to console as well.'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_5ZEDbC9xRELxX62MQgrt6DIb',\n",
       "    'function': {'arguments': '{\\n  \"n\": 8\\n}', 'name': 'Fibonacci'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': '21',\n",
       "  'tool_call_id': 'call_5ZEDbC9xRELxX62MQgrt6DIb'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_Whczy5fQYmrgelRtDYzGZ5Xx',\n",
       "    'function': {'arguments': '{\\n  \"text\": \"63\"\\n}', 'name': 'write'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': 'None',\n",
       "  'tool_call_id': 'call_Whczy5fQYmrgelRtDYzGZ5Xx'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_AaivIquM9AfuSW1dMi3GXyQf',\n",
       "    'function': {'arguments': '{\\n  \"text\": \"63\"\\n}',\n",
       "     'name': 'print_to_console'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': 'success',\n",
       "  'tool_call_id': 'call_AaivIquM9AfuSW1dMi3GXyQf'},\n",
       " {'content': 'I have calculated the 8th Fibonacci number, which is 21. I have added it to the number on your notepad, which is 42, and the result is 63. I have written this number on your notepad and printed it to the console as well.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.messages.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing our ChatGPT Class\n",
    "\n",
    "Our `Assistant` classes are based on the concept of Event Generators, i.e. a python iterator that yields different events. The above `__call__()` method is an example of an Event Handler that simply calls the event generators and reacts to the emitted events it cares about and ignores the rest. This is a very flexible design pattern that allows us to easily customize the behavior of the model.\n",
    "\n",
    "Let's override the `__call__()` method to handle some more events we care about:\n",
    "- let's print the streamed partial responses directly\n",
    "- let's use the jupyter `display` function to display the full response in a nice Markdown format\n",
    "\n",
    "Basically like the following simple toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Hello, world!\n",
       "\n",
       "# How are you?\n",
       "I love earth! And everything on it!\n",
       "\n",
       "```python\n",
       "print(\"Hello, world!\")\n",
       "def fib(n):\n",
       "    if n < 0:\n",
       "         raise ValueError(\"n must be >= 0\")\n",
       "    if n < 2:\n",
       "        return n\n",
       "    return fib(n-1) + fib(n-2)\n",
       "```\n",
       "Here's a list of things I like:\n",
       "- Earth\n",
       "- The Moon\n",
       "- Mars\n",
       "- You\n",
       "- Myself\n",
       "- GPT-Wrapper\n",
       "\n",
       "Anyways, **goodbye, have a nice day**!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def gen_response(txt):\n",
    "    for i in range(len(txt)):\n",
    "        yield txt[:i+1]\n",
    "        time.sleep(0.01)\n",
    "\n",
    "t = \"\"\"\n",
    "Hello, world!\n",
    "\n",
    "# How are you?\n",
    "I love earth! And everything on it!\n",
    "\n",
    "```python\n",
    "print(\"Hello, world!\")\n",
    "def fib(n):\n",
    "    if n < 0:\n",
    "         raise ValueError(\"n must be >= 0\")\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)\n",
    "```\n",
    "Here's a list of things I like:\n",
    "- Earth\n",
    "- The Moon\n",
    "- Mars\n",
    "- You\n",
    "- Myself\n",
    "- GPT-Wrapper\n",
    "\n",
    "Anyways, **goodbye, have a nice day**!\n",
    "\"\"\"\n",
    "\n",
    "for r in gen_response(t):\n",
    "    display(Markdown(r), clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.models import ChatGPT\n",
    "\n",
    "class NotebookGPT(ChatGPT):\n",
    "    async def __call__(self, prompt: str, **kwargs):\n",
    "\n",
    "        output = '' # accumulate output here\n",
    "\n",
    "        async for event in self.response_events(prompt, **kwargs):\n",
    "            match event:\n",
    "                case self.ResponseStartEvent():\n",
    "                    output += f'**[USER]:** {event.prompt}\\n\\n**[GPT]:** '\n",
    "                    display(Markdown(output), clear=True)\n",
    "                \n",
    "                case self.PartialCompletionEvent():\n",
    "                    try:\n",
    "                        if d:=event.chunk.choices[0].delta.content:\n",
    "                            output += d\n",
    "                            display(Markdown(output), clear=True)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "gpt = NotebookGPT(MessageHistory([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[USER]:** Hey! How are you?\n",
       "\n",
       "**[GPT]:** Hello! I am an AI and do not possess emotions, but I am here to assist you. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await gpt(\"Hey! How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[USER]:** Can you give me a python implementation of the fibonacci sequence iteratively?\n",
       "\n",
       "**[GPT]:** Of course! Here's a Python implementation of the Fibonacci sequence using an iterative approach:\n",
       "\n",
       "```python\n",
       "def fibonacci_iterative(n):\n",
       "    if n <= 0:\n",
       "        return []\n",
       "    elif n == 1:\n",
       "        return [0]\n",
       "    elif n == 2:\n",
       "        return [0, 1]\n",
       "    \n",
       "    # Start with the first two numbers of the sequence\n",
       "    fib_seq = [0, 1]\n",
       "\n",
       "    # Generate the Fibonacci sequence up to the desired length\n",
       "    while len(fib_seq) < n:\n",
       "        next_num = fib_seq[-1] + fib_seq[-2]\n",
       "        fib_seq.append(next_num)\n",
       "    \n",
       "    return fib_seq\n",
       "```\n",
       "\n",
       "In this implementation, we check for cases where the sequence length is less than or equal to 0, 1, or 2 to handle special cases. Then, we start with the first two numbers of the sequence (0 and 1) and keep generating the next numbers by summing up the last two numbers. The process continues until we have the desired length of the Fibonacci sequence.\n",
       "\n",
       "You can call the function `fibonacci_iterative(n)` to get the Fibonacci sequence of length `n`. For example, `fibonacci_iterative(10)` would return `[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await gpt(\"Can you give me a python implementation of the fibonacci sequence iteratively?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even more customization\n",
    "\n",
    "So far we've only overriden the Event Handlers, with which you can go pretty far. However, you can also override the Event Generators to customize the behavior even more. You could define new Events and then yield them in your custom overriden Event Generators.\n",
    "\n",
    "This could be useful when you're looking into advanced use cases, such as Code Interpreter with a \"hacked\" python tool, in order to get around the issue of all tool arguments being in JSON format, and the model often makes syntax mistakes in writing long blocks of code while simultaneously encoding it in a JSON string format (escaping new lines and quotes, etc.). For this, you would override the tool text message content handler (currently a TODO).\n",
    "\n",
    "Another example could be for human-in-the-loop tools, e.g. you want the user to explictly confirm the tool call before it's actually executed for critical tools. For this you could yield a new Event called `ConfirmToolCall` and then override the `tool_events` generator to yield this event when needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
