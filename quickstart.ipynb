{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "A tour of this package's functionality! Buckle up because it's gonna be a fun one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Toolkits\n",
    "\n",
    "### Tools: `@function_tool`\n",
    "You can make any function into a tool usable by the model by decorating it with `@function_tool`, but you must do the following:\n",
    "- The parameters must be type annotated\n",
    "- The return type *should* be str, but not required\n",
    "- It must be documented with a docstring, including each parameter (most formats supported, e.g. Google-style, NumPy-style, sphinx-style, etc.)\n",
    "\n",
    "You can use the tool from python as you normally would, but also the annotated tool will be auto-parsed and its JSON schema and argument validator (using pydantic) will be available as attributes on the function:\n",
    "- `tool.schema` is the JSON schema, which you can directly pass to the OpenAI API.\n",
    "- `tool.lookup` is a dict lookup table of the function name to the (auto-validated) function call, meaning you can just receive the model-generated function call and pass it directly to the tool.\n",
    "\n",
    "Other attributes (probably commonly used):\n",
    "- `tool.validator` is the pydantic validator, callable with kwargs to validate the arguments. This is what is saved in the lookup table.\n",
    "- `tool.tool_enabled` is a bool switch to enable/disable the tool. This is useful esp. for Toolkits, where you can dynamically enable/disable tools based on the context.\n",
    "- `tool.name` should ideally not be set directly, but passed as a kwarg to `@function_tool`. It is the name of the tool, as known to the model. If not set, it will be the name of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.tools import Toolkit, function_tool, fail_with_message\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from python!\n",
      "Hello from GPT!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@function_tool\n",
    "def print_to_console(text: str) -> str:\n",
    "    '''\n",
    "    Print text to console\n",
    "\n",
    "    Args:\n",
    "        text: text to print\n",
    "    '''\n",
    "    print(text)\n",
    "    return 'success' # ideally, we always return something to tell the model\n",
    "\n",
    "# normal call\n",
    "print_to_console('Hello from python!')\n",
    "\n",
    "# call from lookup table\n",
    "lookup = print_to_console.lookup\n",
    "# this would be generated by GPT\n",
    "name, arguments = 'print_to_console', {'text': 'Hello from GPT!'}\n",
    "func = lookup[name]\n",
    "func(arguments) # notice the lack of unpacking, we pass the args dict directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Argument: 1 validation error for print_to_console\n",
      "text\n",
      "  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.5/v/string_type\n",
      "Invalid Argument: 1 validation error for print_to_console\n",
      "text\n",
      "  Field required [type=missing, input_value={'content': 'Whats up?'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.5/v/missing\n"
     ]
    }
   ],
   "source": [
    "# if the model generates an invalid argument, it is auto-validated\n",
    "print(func({'text': 123}))\n",
    "print(func({'content': \"Whats up?\"}))\n",
    "\n",
    "# notice that it's returned as string, because it should be passed to the model to correct itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing off some more goodies:\n",
    "- Even async functions should seamlessly work, just don't forget to `await` them.\n",
    "- `@fail_with_message(err)` is a decorator that will catch any exceptions thrown by the function and instead return the error message. This is useful for when you want to handle errors in a more graceful way than just crashing the model. It also takes an optional logger, which by default takes the `print` function, but any callable that takes a string will work, such as `logger.error` from the `logging` module.\n",
    "- Usually, the `@function_tool` decorator will throw an assertion error if you forget to provide the description for any of the function or their parameters. If you really don't want to provide descriptions for some (or all), maybe because it's so self-explanatory or you need to save tokens, then you can explicitly turn off the docstring parsing by passing `@function_tool(check_description=False)`. This is not recommended, but it's there if you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tool call fib(n=-10) failed: n must be >= 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: n must be >= 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'Fibonacci',\n",
       "  'description': 'Calculate the nth Fibonacci number',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'n': {'description': 'The index of the Fibonacci number to calculate',\n",
       "     'type': 'integer'}},\n",
       "   'required': ['n']}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "@function_tool(name=\"Fibonacci\")\n",
    "@fail_with_message(\"Error\", logger=logger.error)\n",
    "async def fib(n: int):\n",
    "    '''\n",
    "    Calculate the nth Fibonacci number\n",
    "\n",
    "    Args:\n",
    "        n: The index of the Fibonacci number to calculate\n",
    "    '''\n",
    "    if n < 0:\n",
    "         raise ValueError(\"n must be >= 0\")\n",
    "    if n < 2:\n",
    "        return n\n",
    "    await asyncio.sleep(0.1)\n",
    "    # return await fib(n-1) + await fib(n-2)\n",
    "    # parallel\n",
    "    return sum(await asyncio.gather(fib(n-1), fib(n-2)))\n",
    "\n",
    "print(await fib(10), flush=True)\n",
    "print(await fib.lookup['Fibonacci']({'n': -10}))\n",
    "fib.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkits: `class Toolkit`\n",
    "Toolkits are a collection of related function tools, esp. useful when they share a state. Also good for keeping the state bound to a single instance of the toolkit, rather than a global state.\n",
    "To create a toolkit, simply subclass `Toolkit` and decorate its methods with `@function_tool`.\n",
    "Enabled tools will be visible in the attributes:\n",
    "- `toolkit.schema` is the JSON schema, which you can directly pass to the OpenAI API.\n",
    "- `toolkit.lookup` is a dict lookup table of the function name to the (auto-validated) function call, meaning you can just receive the model-generated function call and pass it directly to the tool.\n",
    "\n",
    "As you noticed, the interface for OpenAI API is the same for both tools and toolkits, so you can use them interchangeably, and to use multiple tools and toolkits, simply merge their schemas and lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "class Notepad(Toolkit):\n",
    "    def __init__(self):\n",
    "        self.content = \"<Fill me in>\"\n",
    "    \n",
    "    @function_tool\n",
    "    def write(self, text: str):\n",
    "        '''\n",
    "        Write text to the notepad\n",
    "\n",
    "        Args:\n",
    "            text: The text to write\n",
    "        '''\n",
    "        self.content = text\n",
    "    \n",
    "    @function_tool(check_description=False)\n",
    "    def read(self):\n",
    "        return self.content\n",
    "    \n",
    "notes = Notepad()\n",
    "notes.write(\"Hello, world!\")\n",
    "print(notes.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes.lookup['write'](text=\"Shhh... here's a secret: 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shhh... here's a secret: 42\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.lookup['read']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shhh... here's a secret: 42\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT model and MessageHistory\n",
    "\n",
    "### ChatGPT: `class ChatGPT`\n",
    "\n",
    "Simple wrapper that keeps a message history object and optional toolkit (TODO: make into list of toolkits).\n",
    "The object is callable and simply takes a user prompt string and returns the model response string.\n",
    "\n",
    "### MessageHistory: `class MessageHistory`\n",
    "\n",
    "A simple class that keeps track of the message history, including the user and assistnat messages, along with system and tool call results. Special subclasses could be defined for things like:\n",
    "- Rolling history (e.g. last 10 messages)\n",
    "- Dynamic history based on context (e.g. vector embedding)\n",
    "- System message that constantly updates (e.g. time, weather, etc.)\n",
    "- Few-shot learning, i.e. pre-populating the history with some demonstration messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.models import ChatGPT\n",
    "from gpt_wrapper.messages import MessageHistory, msg\n",
    "\n",
    "gpt = ChatGPT(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    toolkit = notes,\n",
    "    messages=MessageHistory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool Call]: read {}\n",
      "[Tool Result]: Shhh... here's a secret: 42\n",
      "[ChatGPT]: On your notepad, there is a secret number: 42.\n",
      "On your notepad, there is a secret number: 42.\n"
     ]
    }
   ],
   "source": [
    "response = await gpt(\"What's on my notepad?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatGPT]: Sure, I can help you with that. Give me a moment.\n",
      "[Tool Call]: write {\n",
      "  \"text\": \"The doubled value is 84.\"\n",
      "}\n",
      "[Tool Result]: None\n",
      "[ChatGPT]: I have doubled the value for you - it is now 84.\n",
      "I have doubled the value for you - it is now 84.\n"
     ]
    }
   ],
   "source": [
    "response = await gpt(\"Can you double it?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The doubled value is 84.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user', 'content': \"What's on my notepad?\"},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_qJizBworyqr7lh3HX9AB2bZF',\n",
       "    'function': {'arguments': '{}', 'name': 'read'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': \"Shhh... here's a secret: 42\",\n",
       "  'tool_call_id': 'call_qJizBworyqr7lh3HX9AB2bZF'},\n",
       " {'content': 'On your notepad, there is a secret number: 42.',\n",
       "  'role': 'assistant'},\n",
       " {'role': 'user', 'content': 'Can you double it?'},\n",
       " {'content': 'Sure, I can help you with that. Give me a moment.',\n",
       "  'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_nhD4CklppuF1gzs0SKl5A5Bl',\n",
       "    'function': {'arguments': '{\\n  \"text\": \"The doubled value is 84.\"\\n}',\n",
       "     'name': 'write'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': 'None',\n",
       "  'tool_call_id': 'call_nhD4CklppuF1gzs0SKl5A5Bl'},\n",
       " {'content': 'I have doubled the value for you - it is now 84.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.messages.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
