{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "A tour of this package's functionality, buckle up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Toolkits\n",
    "\n",
    "### Tools: `@function_tool`\n",
    "You can make any function into a tool usable by the model by decorating it with `@function_tool`, but you must do the following:\n",
    "- The parameters must be type annotated\n",
    "- The return type *should* be str, but not required (currently just stringified)\n",
    "- It must be documented with a `'''docstring'''`, including each parameter (most [formats supported](https://github.com/rr-/docstring_parser), e.g. [Google-style](https://gist.github.com/redlotus/3bc387c2591e3e908c9b63b97b11d24e#file-docstrings-py-L67), [NumPy-style](https://gist.github.com/eikonomega/910512d92769b0cc382a09ae4de41771), sphinx-style, etc, see [this overview](https://gist.github.com/nipunsadvilkar/fec9d2a40f9c83ea7fd97be59261c400))\n",
    "\n",
    "You can use the tool from python as you normally would, but also the annotated tool will be auto-parsed and its JSON schema and argument validator (using pydantic) will be available as attributes on the function:\n",
    "- `tool.schema` is the (singular list of) JSON schema, which you can directly pass to the OpenAI API.\n",
    "- `tool.lookup` is a dict lookup table of the function name to the (auto-validated) function call, meaning you can just receive the model-generated function call and pass it directly to the tool.\n",
    "\n",
    "Other attributes (probably uncommon):\n",
    "- `tool.validator` is the pydantic validator, callable with kwargs to validate the arguments. This is what is saved in the lookup table.\n",
    "- `tool.tool_enabled` is a bool switch to enable/disable the tool. This is useful esp. for Toolkits, where you can dynamically enable/disable tools based on the context. Note that this only works if you wrap the tool in `ToolList` class or is part of a `Toolkit`.\n",
    "- `tool.name` should ideally not be set directly, but passed as a kwarg to `@function_tool`. It is the name of the tool, as known to the model. If not set, it will be the name of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.tools import ToolList, Toolkit, function_tool, fail_with_message\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from python!\n",
      "Hello from GPT!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@function_tool\n",
    "def print_to_console(text: str) -> str:\n",
    "    '''\n",
    "    Print text to console\n",
    "\n",
    "    Args:\n",
    "        text: text to print\n",
    "    '''\n",
    "    print(text)\n",
    "    return 'success' # ideally, we always return something to tell the model\n",
    "\n",
    "# normal call\n",
    "print_to_console('Hello from python!')\n",
    "\n",
    "# call from lookup table\n",
    "lookup = print_to_console.lookup\n",
    "# this would be generated by GPT\n",
    "name, arguments = 'print_to_console', {'text': 'Hello from GPT!'}\n",
    "func = lookup[name]\n",
    "func(arguments) # notice the lack of unpacking, we pass the args dict directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awaiting thread\n",
      "Hello from GPT!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import json\n",
    "from gpt_wrapper.tools import call_requested_function\n",
    "\n",
    "call = SimpleNamespace(name='print_to_console', arguments=json.dumps({'text': 'Hello from GPT!'}))\n",
    "await call_requested_function(call, print_to_console.lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Argument: 1 validation error for print_to_console\n",
      "text\n",
      "  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n",
      "Invalid Argument: 1 validation error for print_to_console\n",
      "text\n",
      "  Field required [type=missing, input_value={'content': 'Whats up?'}, input_type=dict]\n"
     ]
    }
   ],
   "source": [
    "# if the model generates an invalid argument, it is auto-validated\n",
    "print(func({'text': 123}))\n",
    "print(func({'content': \"Whats up?\"}))\n",
    "\n",
    "# notice that it's returned as string, because it should be passed to the model to correct itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing off some more goodies:\n",
    "- Even async functions should seamlessly work, just don't forget to `await` them.\n",
    "- `@fail_with_message(err)` is a decorator that will catch any exceptions thrown by the function and instead return the error message. This is useful for when you want to handle errors in a more graceful way than just crashing the model. It also takes an optional logger, which by default takes the `print` function, but any callable that takes a string will work, such as `logger.error` from the `logging` module.\n",
    "- Usually, the `@function_tool` decorator will throw an assertion error if you forget to provide the description for any of the function or their parameters. If you really don't want to provide descriptions for some (or all), maybe because it's so self-explanatory or you need to save tokens, then you can explicitly turn off the docstring parsing by passing `@function_tool(check_description=False)`. This is not recommended, but it's there if you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tool call fib(n=-10) failed: n must be >= 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: n must be >= 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'Fibonacci',\n",
       "   'description': 'Calculate the nth Fibonacci number',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'n': {'description': 'The index of the Fibonacci number to calculate',\n",
       "      'type': 'integer'}},\n",
       "    'required': ['n']}}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "@function_tool(name=\"Fibonacci\")\n",
    "@fail_with_message(\"Error\", logger=logger.error)\n",
    "async def fib(n: int):\n",
    "    '''\n",
    "    Calculate the nth Fibonacci number\n",
    "\n",
    "    Args:\n",
    "        n: The index of the Fibonacci number to calculate\n",
    "    '''\n",
    "    if n < 0:\n",
    "         raise ValueError(\"n must be >= 0\")\n",
    "    if n < 2:\n",
    "        return n\n",
    "    await asyncio.sleep(0.1)\n",
    "    # return await fib(n-1) + await fib(n-2)\n",
    "    # parallel\n",
    "    return sum(await asyncio.gather(fib(n-1), fib(n-2)))\n",
    "\n",
    "print(await fib(10), flush=True)\n",
    "print(await fib.lookup['Fibonacci']({'n': -10}))\n",
    "fib.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, you might want to manually pass the JSON Schema as function argument schema instead of parsing the docstring for it. To do so, you can pass a keyword argument `json_schema` to the `@function_tool` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'create_song',\n",
       "   'description': 'Create a song',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'song': {'allOf': [{'properties': {'title': {'type': 'string'},\n",
       "         'genres': {'items': {'type': 'string'}, 'type': 'array'},\n",
       "         'duration': {'type': 'number'}},\n",
       "        'required': ['title', 'genres', 'duration'],\n",
       "        'type': 'object'}],\n",
       "      'description': 'song to add'}},\n",
       "    'required': ['song']}}}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "class Song(BaseModel):\n",
    "    title: str\n",
    "    genres: list[str]\n",
    "    duration: float\n",
    "\n",
    "@function_tool\n",
    "def create_song(song: Song):\n",
    "    '''\n",
    "    Create a song\n",
    "\n",
    "    Args:\n",
    "        song: song to add\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "create_song.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'append_song',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'title': {'type': 'string'},\n",
       "     'genres': {'items': {'type': 'string'}, 'type': 'array'},\n",
       "     'duration': {'type': 'number'}},\n",
       "    'required': ['title', 'genres', 'duration']}}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "class Song(BaseModel):\n",
    "    title: str\n",
    "    genres: list[str]\n",
    "    duration: float\n",
    "\n",
    "@function_tool(require_doc=False)\n",
    "def append_song(title: str, genres: list[str], duration: float):\n",
    "    pass\n",
    "\n",
    "append_song.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Argument: 1 validation error for append_song\n",
      "duration\n",
      "  Field required [type=missing, input_value={'title': 'hi', 'genres': []}, input_type=dict]\n"
     ]
    }
   ],
   "source": [
    "print(append_song.lookup['append_song']({\"title\":'hi', \"genres\":[]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'create_song2',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'title': {'type': 'string'},\n",
       "     'genres': {'items': {'type': 'string'}, 'type': 'array'},\n",
       "     'duration': {'type': 'number'}},\n",
       "    'required': ['title', 'genres', 'duration']}}}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@function_tool(json_schema=Song.model_json_schema())\n",
    "def create_song2(**song):\n",
    "    pass\n",
    "\n",
    "create_song2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Argument: 123 is not of type 'string'\n",
      "'123.5' is not of type 'number'\n"
     ]
    }
   ],
   "source": [
    "print(create_song2.lookup['create_song2']({\"title\":'hi', \"genres\":[123], \"duration\": \"123.5\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkits: `class Toolkit`\n",
    "Toolkits are a collection of related function tools, esp. useful when they share a state. Also good for keeping the state bound to a single instance of the toolkit, rather than a global state.\n",
    "To create a toolkit, simply subclass `Toolkit` and decorate its methods with `@function_tool`.\n",
    "Enabled tools will be visible in the attributes:\n",
    "- `toolkit.schema` is the JSON schema, which you can directly pass to the OpenAI API.\n",
    "- `toolkit.lookup` is a dict lookup table of the function name to the (auto-validated) function call, meaning you can just receive the model-generated function call and pass it directly to the tool.\n",
    "\n",
    "As you noticed, the interface for OpenAI API is the same for both tools and toolkits, so you can use them interchangeably, and to use multiple tools and toolkits, simply merge their schemas and lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "class Notepad(Toolkit):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.content = \"<Fill me in>\"\n",
    "    \n",
    "    @function_tool\n",
    "    def write(self, text: str):\n",
    "        '''\n",
    "        Write text to the notepad\n",
    "\n",
    "        Args:\n",
    "            text: The text to write\n",
    "        '''\n",
    "        self.content = text\n",
    "    \n",
    "    @function_tool(require_doc=False)\n",
    "    def read(self):\n",
    "        return self.content\n",
    "    \n",
    "notes = Notepad()\n",
    "notes.write(\"Hello, world!\")\n",
    "print(notes.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shhh... here's a secret: 42\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.lookup['write']({'text': \"Shhh... here's a secret: 42\"})\n",
    "notes.lookup['read']({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT model and MessageHistory\n",
    "\n",
    "### ChatGPT: `class ChatGPT`\n",
    "\n",
    "Simple wrapper that keeps a message history object and optional toolkit (TODO: make into list of toolkits).\n",
    "The object is callable and simply takes a user prompt string and returns the model response string.\n",
    "\n",
    "### MessageHistory: `class MessageHistory`\n",
    "\n",
    "A simple class that keeps track of the message history, including the user and assistnat messages, along with system and tool call results. Special subclasses could be defined for things like:\n",
    "- Rolling history (e.g. last 10 messages)\n",
    "- Dynamic history based on context (e.g. vector embedding)\n",
    "- System message that constantly updates (e.g. time, weather, etc.)\n",
    "- Few-shot learning, i.e. pre-populating the history with some demonstration messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.assistants import ChatGPT\n",
    "from gpt_wrapper.messages import SimpleHistory, msg\n",
    "\n",
    "gpt = ChatGPT(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    tools = ToolList(notes, print_to_console, fib),\n",
    "    messages = SimpleHistory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[User]: What's on my notepad?\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_lW9D5ugG0ab8jUyA3OcbaDjg', function=Function(arguments='', name='read'), type='function')]\n",
      "[Tool Result]: Error: Failed to parse arguments, make sure your arguments is a valid JSON object: Expecting value: line 1 column 1 (char 0)\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_lW9D5ugG0ab8jUyA3OcbaDjg', function=Function(arguments='{}', name='read'), type='function')]\n",
      "awaiting thread\n",
      "[Tool Result]: Shhh... here's a secret: 42\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_lW9D5ugG0ab8jUyA3OcbaDjg', function=Function(arguments='{}', name='read'), type='function')]\n",
      "awaiting thread\n",
      "[Tool Result]: Shhh... here's a secret: 42\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m gpt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms on my notepad?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/proj/gpt-wrapper/gpt_wrapper/assistants.py:132\u001b[0m, in \u001b[0;36mChatGPT.__call__\u001b[0;34m(self, prompt, tools, model, max_function_calls, **openai_kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, tools: Optional[Tools] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, model: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, max_function_calls: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Prompt the assistant, returning its final response.'''\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_events(prompt, tools, model, max_function_calls, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_kwargs):\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mmatch\u001b[39;00m event:\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mResponseStartEvent():\n\u001b[1;32m    135\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[User]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[38;5;66;03m# print(f\"Unhandled Event: {unhandled_event}\")\u001b[39;00m\n\u001b[1;32m    152\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/proj/gpt-wrapper/gpt_wrapper/assistants.py:184\u001b[0m, in \u001b[0;36mChatGPT.response_events\u001b[0;34m(self, prompt, tools, model, max_function_calls, parallel_calls, **openai_kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m completion_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_events(\n\u001b[1;32m    173\u001b[0m     call_index,\n\u001b[1;32m    174\u001b[0m     tools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m completion: ChatCompletion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m partial_event \u001b[38;5;129;01min\u001b[39;00m completion_events:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m partial_event\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m partial_event:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCompletionEvent():\n\u001b[1;32m    188\u001b[0m             completion \u001b[38;5;241m=\u001b[39m partial_event\u001b[38;5;241m.\u001b[39mcompletion\n",
      "File \u001b[0;32m~/proj/gpt-wrapper/gpt_wrapper/assistants.py:221\u001b[0m, in \u001b[0;36mChatGPT.completion_events\u001b[0;34m(self, call_index, tools, parallel_calls, openai_args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mModel API call to generate the response from the prompt, always yielding the full completion object at the end.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03mThis implementation does streaming generation, yielding partial completions as they come in.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03mYou should override this to customize the stream handling, also defining your own events to yield.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# call the underlying API\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m completion_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m openai_chat(stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_args)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# yield events during streaming\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk, partial \u001b[38;5;129;01min\u001b[39;00m accumulate_partial(completion_stream):\n",
      "File \u001b[0;32m~/proj/gpt-wrapper/gpt_wrapper/api.py:36\u001b[0m, in \u001b[0;36mopenai_chat\u001b[0;34m(**openai_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# new client for each call (TODO: is this efficient?)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m client \u001b[38;5;241m=\u001b[39m AsyncOpenAI()\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/openai/resources/chat/completions.py:1295\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1297\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1298\u001b[0m             {\n\u001b[1;32m   1299\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1300\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1301\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1302\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1303\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1306\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1307\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1308\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1309\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1310\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1311\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1312\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1313\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1314\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1315\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1316\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1317\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1318\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1319\u001b[0m             },\n\u001b[1;32m   1320\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1321\u001b[0m         ),\n\u001b[1;32m   1322\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1323\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1324\u001b[0m         ),\n\u001b[1;32m   1325\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1326\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1327\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1328\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/openai/_base_client.py:1536\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1524\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1532\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1533\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1534\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[0;32m-> 1536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/openai/_base_client.py:1315\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1308\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1316\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1317\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1318\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1319\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1320\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1321\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.11/site-packages/openai/_base_client.py:1392\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m   1390\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1395\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1396\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1399\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1400\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}"
     ]
    }
   ],
   "source": [
    "response = await gpt(\"What's on my notepad?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[User]: Can you calculate the 8th fibonacci number, add it to the number in my notes, and write it? also print it to console as well.\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_DX5j3njmddwBxxbZdX7urrqF', function=Function(arguments='{\\n  \"n\": 8\\n}', name='Fibonacci'), type='function')]\n",
      "awaiting coroutine\n",
      "[Tool Result]: 21\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_0scDGlD3V63qk51NMySRG7dI', function=Function(arguments='{\\n  \"text\": \"63\"\\n}', name='write'), type='function')]\n",
      "awaiting thread\n",
      "[Tool Result]: None\n",
      "[Tool Calls]: [ChatCompletionMessageToolCall(id='call_Uh5Y8fKhH4sEgTAD1e2UN129', function=Function(arguments='{\\n  \"text\": \"63\"\\n}', name='print_to_console'), type='function')]\n",
      "awaiting thread\n",
      "63\n",
      "[Tool Result]: success\n",
      "[ChatGPT]: I have calculated the 8th Fibonacci number, which is 21. I have added it to the number in your notes, which is 42, and the result is 63.\n",
      "\n",
      "I have written the result, 63, on your notepad.\n",
      "[Final Response]: I have calculated the 8th Fibonacci number, which is 21. I have added it to the number in your notes, which is 42, and the result is 63.\n",
      "\n",
      "I have written the result, 63, on your notepad.\n"
     ]
    }
   ],
   "source": [
    "response = await gpt(\"Can you calculate the 8th fibonacci number, add it to the number in my notes, and write it? also print it to console as well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'63'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant'},\n",
       " {'role': 'user', 'content': \"What's on my notepad?\"},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_CPbP6zMQH5eP0PgV5WJAC26N',\n",
       "    'function': {'arguments': '{}', 'name': 'read'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': \"Shhh... here's a secret: 42\",\n",
       "  'tool_call_id': 'call_CPbP6zMQH5eP0PgV5WJAC26N'},\n",
       " {'content': 'On your notepad, there is a secret written: 42',\n",
       "  'role': 'assistant'},\n",
       " {'role': 'user',\n",
       "  'content': 'Can you calculate the 8th fibonacci number, add it to the number in my notes, and write it? also print it to console as well.'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_DX5j3njmddwBxxbZdX7urrqF',\n",
       "    'function': {'arguments': '{\\n  \"n\": 8\\n}', 'name': 'Fibonacci'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': '21',\n",
       "  'tool_call_id': 'call_DX5j3njmddwBxxbZdX7urrqF'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_0scDGlD3V63qk51NMySRG7dI',\n",
       "    'function': {'arguments': '{\\n  \"text\": \"63\"\\n}', 'name': 'write'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': 'None',\n",
       "  'tool_call_id': 'call_0scDGlD3V63qk51NMySRG7dI'},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_Uh5Y8fKhH4sEgTAD1e2UN129',\n",
       "    'function': {'arguments': '{\\n  \"text\": \"63\"\\n}',\n",
       "     'name': 'print_to_console'},\n",
       "    'type': 'function'}]},\n",
       " {'role': 'tool',\n",
       "  'content': 'success',\n",
       "  'tool_call_id': 'call_Uh5Y8fKhH4sEgTAD1e2UN129'},\n",
       " {'content': 'I have calculated the 8th Fibonacci number, which is 21. I have added it to the number in your notes, which is 42, and the result is 63.\\n\\nI have written the result, 63, on your notepad.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.messages.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing our ChatGPT Class\n",
    "\n",
    "Our `Assistant` classes are based on the concept of Event Generators, i.e. a python iterator that yields different events. The above `__call__()` method is an example of an Event Handler that simply calls the event generators and reacts to the emitted events it cares about and ignores the rest. This is a very flexible design pattern that allows us to easily customize the behavior of the model.\n",
    "\n",
    "Let's override the `__call__()` method to handle some more events we care about:\n",
    "- let's print the streamed partial responses directly\n",
    "- let's use the jupyter `display` function to display the full response in a nice Markdown format\n",
    "\n",
    "Basically like the following simple toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Hello, world!\n",
       "\n",
       "# How are you?\n",
       "I love earth! And everything on it!\n",
       "\n",
       "```python\n",
       "print(\"Hello, world!\")\n",
       "def fib(n):\n",
       "    if n < 0:\n",
       "         raise ValueError(\"n must be >= 0\")\n",
       "    if n < 2:\n",
       "        return n\n",
       "    return fib(n-1) + fib(n-2)\n",
       "```\n",
       "Here's a list of things I like:\n",
       "- Earth\n",
       "- The Moon\n",
       "- Mars\n",
       "- You\n",
       "- Myself\n",
       "- GPT-Wrapper\n",
       "\n",
       "Anyways, **goodbye, have a nice day**!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def gen_response(txt):\n",
    "    for i in range(len(txt)):\n",
    "        yield txt[:i+1]\n",
    "        time.sleep(0.01)\n",
    "\n",
    "t = \"\"\"\n",
    "Hello, world!\n",
    "\n",
    "# How are you?\n",
    "I love earth! And everything on it!\n",
    "\n",
    "```python\n",
    "print(\"Hello, world!\")\n",
    "def fib(n):\n",
    "    if n < 0:\n",
    "         raise ValueError(\"n must be >= 0\")\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)\n",
    "```\n",
    "Here's a list of things I like:\n",
    "- Earth\n",
    "- The Moon\n",
    "- Mars\n",
    "- You\n",
    "- Myself\n",
    "- GPT-Wrapper\n",
    "\n",
    "Anyways, **goodbye, have a nice day**!\n",
    "\"\"\"\n",
    "\n",
    "for r in gen_response(t):\n",
    "    display(Markdown(r), clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.assistants import ChatGPT\n",
    "\n",
    "class NotebookGPT(ChatGPT):\n",
    "    async def __call__(self, prompt: str, **kwargs):\n",
    "        output = ''\n",
    "        async for event in self.response_events(prompt, **kwargs):\n",
    "            match event:\n",
    "                case self.ResponseStartEvent():\n",
    "                    output += f'**[USER]:** {event.prompt}\\n\\n**[GPT]:** '\n",
    "                    display(Markdown(output), clear=True)\n",
    "                \n",
    "                case self.PartialCompletionEvent():\n",
    "                    try:\n",
    "                        if d:=event.chunk.choices[0].delta.content:\n",
    "                            output += d\n",
    "                            display(Markdown(output), clear=True)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "gpt = NotebookGPT(SimpleHistory([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[USER]:** Hey! How are you?\n",
       "\n",
       "**[GPT]:** Hello! I'm an artificial intelligence created by OpenAI, so I don't have feelings, but I'm here to assist you. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await gpt(\"Hey! How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**[USER]:** Can you give me a python implementation of the fibonacci sequence iteratively?\n",
       "\n",
       "**[GPT]:** Sure! Here's a Python implementation of the Fibonacci sequence using an iterative approach:\n",
       "\n",
       "```python\n",
       "def fibonacci_iterative(n):\n",
       "    first = 0\n",
       "    second = 1\n",
       "    \n",
       "    if n <= 0:\n",
       "        return \"Please enter a positive integer.\"\n",
       "    elif n == 1:\n",
       "        return first\n",
       "    elif n == 2:\n",
       "        return second\n",
       "    else:\n",
       "        for i in range(3, n+1):\n",
       "            next_num = first + second\n",
       "            first = second\n",
       "            second = next_num\n",
       "        return second\n",
       "\n",
       "# Testing the function\n",
       "num = int(input(\"Enter a positive integer: \"))\n",
       "result = fibonacci_iterative(num)\n",
       "print(f\"The {num}th number in the Fibonacci sequence is: {result}\")\n",
       "```\n",
       "\n",
       "In this implementation, we use a loop to compute the Fibonacci sequence iteratively. We start with the first two numbers (`first` and `second`), and in each iteration, we calculate the next number by adding the previous two numbers. Finally, we return the `n`th number in the sequence.\n",
       "\n",
       "Note: In this code, we assume that the Fibonacci sequence starts with 0. If you want it to start with 1, you can adjust the initial values of `first` and `second` accordingly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await gpt(\"Can you give me a python implementation of the fibonacci sequence iteratively?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even more customization\n",
    "\n",
    "So far we've only overriden the Event Handlers, with which you can go pretty far. However, you can also override the Event Generators to customize the behavior even more. You could define new Events and then yield them in your custom overriden Event Generators.\n",
    "\n",
    "This could be useful when you're looking into advanced use cases, such as Code Interpreter with a \"hacked\" python tool, in order to get around the issue of all tool arguments being in JSON format, and the model often makes syntax mistakes in writing long blocks of code while simultaneously encoding it in a JSON string format (escaping new lines and quotes, etc.). For this, you would override the tool text message content handler (currently a TODO).\n",
    "\n",
    "Another example could be for human-in-the-loop tools, e.g. you want the user to explictly confirm the tool call before it's actually executed for critical tools. For this you could yield a new Event called `ConfirmToolCall` and then override the `tool_events` generator to yield this event when needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
