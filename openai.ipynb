{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API and nice-to-have wrappers\n",
    "\n",
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla API: Synchronous Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# API call parameters\n",
    "from openai.types.chat.chat_completion_message_param import (\n",
    "    ChatCompletionMessageParam, # i.e. any message\n",
    "    ChatCompletionSystemMessageParam, # system message\n",
    "    ChatCompletionUserMessageParam, # user message\n",
    "    ChatCompletionAssistantMessageParam, # assistant message (generated by model)\n",
    "    ChatCompletionToolMessageParam, # result of tool call\n",
    ")\n",
    "# API return values\n",
    "from openai.types.chat import (\n",
    "    ChatCompletion, # Overall Completion, has id, stats, choices\n",
    "    ChatCompletionMessage, # completion.choice[0], has role, content, tool_calls\n",
    ")\n",
    "from openai.types.chat.chat_completion_message_tool_call import (\n",
    "    ChatCompletionMessageToolCall, # a tool call with id, function and type (only function is supported)\n",
    "    Function, # function call with name and arguments\n",
    ")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'NoneType' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jw/proj/gpt-wrapper/openai.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw/proj/gpt-wrapper/openai.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# lookup function, parse args, call them, then return result\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw/proj/gpt-wrapper/openai.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msuccess\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jw/proj/gpt-wrapper/openai.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandle_completion\u001b[39m(completion: ChatCompletion) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;49;00m \u001b[39m|\u001b[39;49m [ChatCompletionToolMessageParam]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw/proj/gpt-wrapper/openai.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(completion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw/proj/gpt-wrapper/openai.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# Whatever we want to actually do with the response\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jw/proj/gpt-wrapper/openai.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# handle message\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'NoneType' and 'list'"
     ]
    }
   ],
   "source": [
    "kwargs = dict(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    ")\n",
    "\n",
    "def handle_function_call(call: Function) -> str:\n",
    "    print(f\"calling {call.name} with {call.arguments}\")\n",
    "    # lookup function, parse args, call them, then return result\n",
    "    return 'success'\n",
    "\n",
    "def handle_completion(completion: ChatCompletion) -> None | [ChatCompletionToolMessageParam]:\n",
    "    print(completion)\n",
    "\n",
    "    # Whatever we want to actually do with the response\n",
    "    # handle message\n",
    "    message = completion.choices[0].message\n",
    "\n",
    "    # handle tool calls\n",
    "    calls = message.tool_calls\n",
    "    if calls:\n",
    "        results: list[ChatCompletionToolMessageParam] = []\n",
    "        for call in calls:\n",
    "            if call.type == \"function\":\n",
    "                result = handle_function_call(call.function)\n",
    "                # Note that here, we would usually append the result to the message list for the next call, instead return\n",
    "                results.append(ChatCompletionToolMessageParam(\n",
    "                    tool_call_id=call.id, # for parallel calls\n",
    "                    role='tool',\n",
    "                    content=result,\n",
    "                ))\n",
    "        return results # tool call results\n",
    "    else:\n",
    "        return None # nothing was called\n",
    "\n",
    "MAX_CALLS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Streaming\n",
    "\n",
    "As Vanila as it gets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8NK7UCIJUuuGfqlNWPyauxRRm6Ffr', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='This is a test', role='assistant', function_call=None, tool_calls=None))], created=1700569576, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=4, prompt_tokens=12, total_tokens=16))\n"
     ]
    }
   ],
   "source": [
    "completion: ChatCompletion = client.chat.completions.create(**kwargs)\n",
    "# do something with completion, i.e. messages, tool calls\n",
    "handle_completion(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the PyDantic models to dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-8NK7UCIJUuuGfqlNWPyauxRRm6Ffr',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'message': {'content': 'This is a test',\n",
       "    'role': 'assistant',\n",
       "    'function_call': None,\n",
       "    'tool_calls': None}}],\n",
       " 'created': 1700569576,\n",
       " 'model': 'gpt-3.5-turbo-1106',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': 'fp_eeff13170a',\n",
       " 'usage': {'completion_tokens': 4, 'prompt_tokens': 12, 'total_tokens': 16}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple back-and-forth conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT: Hi!\n",
      "[{'role': 'user', 'content': 'say \"hi\"'}, ChatCompletionMessage(content='Hi!', role='assistant', function_call=None, tool_calls=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT: Hi! Hi!\n",
      "[{'role': 'user', 'content': 'say \"hi\"'}, ChatCompletionMessage(content='Hi!', role='assistant', function_call=None, tool_calls=None), {'role': 'user', 'content': 'again, but twice!'}, ChatCompletionMessage(content='Hi! Hi!', role='assistant', function_call=None, tool_calls=None)]\n"
     ]
    }
   ],
   "source": [
    "kwargs_without_messages = kwargs.copy()\n",
    "del kwargs_without_messages['messages']\n",
    "\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    message = input(\"You: \")\n",
    "    if message == 'exit':\n",
    "        break\n",
    "    messages.append(ChatCompletionUserMessageParam(role='user', content=message))\n",
    "\n",
    "    completion: ChatCompletion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        **kwargs_without_messages,\n",
    "    )\n",
    "    response: ChatCompletionMessage = completion.choices[0].message\n",
    "    print(f\"ChatGPT: {response.content}\")\n",
    "    messages.append(response)\n",
    "    print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict,\n",
       " openai.types.chat.chat_completion_message.ChatCompletionMessage,\n",
       " dict,\n",
       " openai.types.chat.chat_completion_message.ChatCompletionMessage]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(m) for m in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say \"hi\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletionMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jw/proj/openai.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jw/proj/openai.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jw/proj/openai.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(m[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletionMessage' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for m in messages:\n",
    "    print(m['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, since the API returns a Pydantic Model, we need to convert it into a dict before appending it to the `messages`` list (which gets fed back into the API). The API does deal with it gracefully, but it is confusing for us from a user perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calls, implemented in a simple for loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not that trivial to define the function schema, so see below for much easier auto-convert options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_without_msg = kwargs.copy()\n",
    "messages = kwargs_without_msg.pop('messages')\n",
    "\n",
    "for i in range(MAX_CALLS):\n",
    "    completion: ChatCompletion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        **kwargs_without_msg\n",
    "    )\n",
    "    # do something with completion, i.e. messages, tool calls\n",
    "    tool_call_results = handle_completion()\n",
    "\n",
    "    if tool_call_results:\n",
    "        messages.append(tool_call_results)\n",
    "    else:\n",
    "        # IMPORTANT no more tool calls, we're done\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def generate_tool_calls(**kwargs, max_calls=MAX_CALLS):\n",
    "#     for i in range(max_calls):\n",
    "#         completion = client.chat.completions.create(**kwargs)\n",
    "#         yield completion\n",
    "#         if completion contains no tool calls:\n",
    "#             break\n",
    "            \n",
    "# completions: Generator[ChatCompletion] = generate_tool_calls(client, **kwargs)\n",
    "# for completion in completions:\n",
    "#     # do something with completion, i.e. messages, tool calls\n",
    "#     handle_completion()\n",
    "\n",
    "\n",
    "# [streaming, generator that yields partial completions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice-To-Haves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History Management\n",
    "While not the hardest thing to manage, keeping a `messages` list object directly can lead to some annoyances and bugs, because:\n",
    "1. You need to always do in-place operations to make sure the reference to the original list is kept.\n",
    "2. The return value of API is a PyDantic model ChatCompletionMessage, which is not a dict, leading to inconsistent types.\n",
    "3. When managing long conversation history, you might want to start truncating really old messages or filter out irrelevant parts (using sementic similarity, for example) to stay under context size limitations and also save tokens.\n",
    "4. When using RAG, you could define the user-prompt preprocessing step here.\n",
    "5. When using more complicated system message scheme that keeps track of global state (a trick I like to use in copilots), you want to be able to define the system message creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.messages import MessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = MessageHistory()\n",
    "messages.add_user(\"Hello\")\n",
    "messages.add_assistant({'role': 'AI', 'message': \"Hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Useage Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.trackers import ChatCompletionUsageTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkit Class\n",
    "\n",
    "For autoconverting, validating, and also for any shared states for a group of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from gpt_wrapper.tools import FunctionTool, Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'get_weather',\n",
       "  'description': 'return temp in Fahrenheit',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'location': {'description': 'Location to get weather for',\n",
       "     'properties': {'city': {'type': 'string'}, 'state': {'type': 'string'}},\n",
       "     'required': ['city', 'state'],\n",
       "     'type': 'object'}},\n",
       "   'required': ['location']}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example tool\n",
    "class Location(BaseModel):\n",
    "    '''Location to get weather for'''\n",
    "    city: str\n",
    "    state: str\n",
    "\n",
    "class get_weather(FunctionTool):\n",
    "    '''return temp in Fahrenheit'''\n",
    "    location: Location = Field(..., description=\"Location to get weather for\")\n",
    "\n",
    "    def __call__(args, state=None):\n",
    "        # call weather api\n",
    "        return f'Weather at {args.location} is 72F' if args.location else 'Error: no location provided'\n",
    "\n",
    "get_weather.to_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'read',\n",
       "   'description': 'read the notepad',\n",
       "   'parameters': {'type': 'object', 'properties': {}, 'required': []}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'write',\n",
       "   'description': 'write to the notepad',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'text': {'description': 'text to write to notepad',\n",
       "      'type': 'string'}},\n",
       "    'required': ['text']}}}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NotepadState(BaseModel):\n",
    "    content: str = ''\n",
    "\n",
    "class NotepadToolkit(Toolkit):\n",
    "    def __init__(self):\n",
    "        super().__init__(tools=[\n",
    "            self.read,\n",
    "            self.write,\n",
    "        ])\n",
    "        self.state: NotepadState = NotepadState()\n",
    "    \n",
    "    class read(FunctionTool):\n",
    "        '''read the notepad'''\n",
    "        def __call__(args, state: NotepadState):\n",
    "            return state.content\n",
    "    \n",
    "    class write(FunctionTool):\n",
    "        '''write to the notepad'''\n",
    "        text: str = Field(..., description=\"text to write to notepad\")\n",
    "        async def __call__(args, state: NotepadState):\n",
    "            state.content += args.text\n",
    "            return \"success!\"\n",
    "\n",
    "toolkit = NotepadToolkit()\n",
    "toolkit.to_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New content: hello'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = toolkit.to_tool_lookup()\n",
    "w = lookup['write']\n",
    "await w(text='hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see, async or not, it can be handled the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = lookup['read']\n",
    "r()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling with Auto-Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.assistant import call_requested_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call Request: Function(arguments='{\"text\": \"hi\"}', name='write')\n",
      "awaiting coroutine\n",
      "Tool Result: success!\n",
      "Call Request: Function(arguments='{}', name='read')\n",
      "Tool Result: Surprise! There was already something written here!\n",
      "hi\n",
      "ChatGPT: The content of the notepad is \"Surprise! There was already something written here! hi\".\n"
     ]
    }
   ],
   "source": [
    "notepad = NotepadToolkit()\n",
    "notepad.state.content = \"Surprise! There was already something written here!\\n\"\n",
    "tools = notepad.to_openai()\n",
    "lookup = notepad.to_tool_lookup()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write hi to the notepad, and the read the whole content back to me, verbatim!\"},\n",
    "]\n",
    "\n",
    "for i in range(10):\n",
    "    completion: ChatCompletion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "    )\n",
    "    \n",
    "    response: ChatCompletionMessage = completion.choices[0].message\n",
    "    messages.append(response)\n",
    "\n",
    "    if response.content:\n",
    "        # handle message\n",
    "        print(f\"ChatGPT: {response.content}\")\n",
    "    if response.tool_calls:\n",
    "        for call in response.tool_calls:\n",
    "            # handle tool calls (usually nothing more than logging/streaming)\n",
    "            print(f\"Call Request: {call.function}\")\n",
    "            result = await call_requested_function(call.function, lookup)\n",
    "            print(f\"Tool Result: {result}\")\n",
    "            messages.append(ChatCompletionToolMessageParam(\n",
    "                tool_call_id=call.id, # for parallel calls\n",
    "                role='tool',\n",
    "                content=result,\n",
    "            ))\n",
    "    else:\n",
    "        # IMPORTANT no more tool calls, we're done\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
